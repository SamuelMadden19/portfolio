{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST CNN With Keras\n",
    "#### Created by: Samuel Madden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.5 (default, Mar 30 2018, 06:41:53) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)]\n",
      "Pandas Version: 0.22.0\n"
     ]
    }
   ],
   "source": [
    "# default imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "# Keras imports\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "# Ouptut System Versions\n",
    "print(\"Python Version: \" + sys.version)\n",
    "print(\"Pandas Version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is an image dataset of handwritten digits made available by Yann LeCun et. al. <a href = \"http://yann.lecun.com/exdb/mnist/\">here</a>. It has has 60,000 training images and 10,000 test images, each of which are grayscale 28 x 28 sized images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .read_csv commands if dataset is local\n",
    "\n",
    "#mnist_train = pd.read_csv('mnist/mnist_train.csv')\n",
    "#mnist_test = pd.read_csv('mnist/mnist_test.csv')\n",
    "\n",
    "# Import dataset using Keras Library\n",
    "from keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#X_train, Y_train = mnist_train.iloc[:,1:], mnist_train['label']\n",
    "#X_test, Y_test = mnist_test.iloc[:, 1:], mnist_test['label']\n",
    "\n",
    "(X_train, Y_train),(X_test, Y_test) = load_data()\n",
    "\n",
    "g = sns.countplot(Y_train)\n",
    "\n",
    "np.unique(Y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All labels (0-9) are accounted for in the above value count with \n",
    "no unexpected Null or -1 instances.\n",
    "\n",
    "The labels between 0 - 9 are also relatively evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# note dimension of data images\n",
    "# img_rows, img_cols = 28, 28\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scan for Null/Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.isnan(X_train)\n",
    "#np.isnan(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmds if data was in Pandas.DataFrame format\n",
    "#X_test.isnull().any().describe()\n",
    "#X_test.isnull().any().descrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a grayscale normalization: by default the image set is grayscale\n",
    "\n",
    "Convert the pixel lumination from a value between 0 - 255 to a scale of 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train / 255.0\n",
    "#X_test = X_test / 255.0\n",
    "\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data images are 28px X 28px. These images are reshaped to 2D representation of 1 x 784\n",
    "in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28, width = 28, chanal = 1)\n",
    "\n",
    "# check image shape\n",
    "#X_train[0].shape\n",
    "\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "num_channels = 1 #images are grayscale\n",
    "\n",
    "\n",
    "#X_train = X_train.values.reshape(-1, img_rows, img_cols, 1)\n",
    "X_trainK = np.reshape(X_train,(X_train.shape[0],img_rows, img_cols, num_channels))\n",
    "#X_test = X_test.values.reshape(-1, img_rows, img_cols, 1)\n",
    "X_testK = np.reshape(X_test,(X_test.shape[0],img_rows, img_cols, num_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "Keras requires a 3 dimensional input to correspond to height, width and channel.</br>\n",
    "In the case of MNIST, the images are grayscale and require one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train info: \n",
      "Type:  <class 'numpy.ndarray'>\n",
      "Dimensions:  3\n",
      "Type var stored:  float32\n",
      "X_trainK shape at [1]:  (60000, 28, 28, 1)\n",
      "\n",
      "\n",
      "\n",
      "X_test info: \n",
      "Type:  <class 'numpy.ndarray'>\n",
      "Dimensions:  3\n",
      "Type var stored:  float32\n",
      "X_testK shape:  (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# X_train\n",
    "print(\"X_train info: \")\n",
    "print(\"Type: \", type(X_train))\n",
    "print(\"Dimensions: \", X_train.ndim)\n",
    "print(\"Type var stored: \", X_train.dtype)\n",
    "print(\"X_trainK shape at [1]: \", X_trainK.shape)\n",
    "print(\"\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# X_test\n",
    "print(\"X_test info: \")\n",
    "print(\"Type: \", type(X_test))\n",
    "print(\"Dimensions: \", X_test.ndim)\n",
    "print(\"Type var stored: \", X_test.dtype)\n",
    "print(\"X_testK shape: \", X_testK.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are 10 single digits 0 - 9. </br>\n",
    "&#09;&nbsp;&bull; Convert each digit to a corresponding hot vector ex. 2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]. This allows for identification due to the corresponding activation of one of the 10 output neurons.</br>\n",
    "&nbsp;&bull; Again this transforms the data from type pandas.DataFrame to a np.ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "Y_train = to_categorical(Y_train, num_classes)\n",
    "Y_test = to_categorical(Y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train info: \n",
      "Type:  <class 'numpy.ndarray'>\n",
      "Dimensions:  2\n",
      "Type var stored:  float32\n",
      "Y_train shape:  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Y_train\n",
    "print(\"Y_train info: \")\n",
    "print(\"Type: \", type(Y_train))\n",
    "print(\"Dimensions: \", Y_train.ndim)\n",
    "print(\"Type var stored: \", Y_train.dtype)\n",
    "print(\"Y_train shape: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Splitting Training and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&bull; 10% is partitioned to the evaluation set while the remaining 90% is used to train the model. </br>\n",
    "&bull; A random split of the training data is admissable as we have already established that the distribution of labels is consistent across the data set (see 2.1). This ensures no overrepresentation of certain labels in the X_train/Y_train and X_val/Y_val.</br>\n",
    "&bull; This is not always the case and for unbalanced datasets use stratify = True option in train_test_split function (Only for >=0.17 sklearn versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random_seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and validation sets for fitting the model\n",
    "X_trainData, X_val, Y_trainData, Y_val = train_test_split(X_trainK, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trainData shape:  (54000, 28, 28, 1)\n",
      "Y_trainData shape:  (54000, 10)\n",
      "X_val shape:  (6000, 28, 28, 1)\n",
      "Y_val shape:  (6000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_trainData shape: \", X_trainData.shape)\n",
    "print(\"Y_trainData shape: \", Y_trainData.shape)\n",
    "print(\"X_val shape: \", X_val.shape)\n",
    "print(\"Y_val shape: \", Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a sample element of the X_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras data model type will be set to 'Sequential', in which the model is built one layer at a time.\n",
    "\n",
    "&nbsp; &bull; add() function will add layers to the model individually<br>\n",
    "&nbsp; &bull; The inputs of 32 & 64 refer to the number of nodes in specific layer\n",
    "\n",
    "\n",
    "Conv2D layers are convolutional layers that deal with the 2 dimensional matrix input (a.k.a. input images).<br> \n",
    "This is a learnable filter that is a transformation on the image defined by the kernel size\n",
    "\n",
    "&nbsp; &bull; Kernel size refers to the size of the filter matrix for the convolutional layer. Size=3 means a 3x3 conv matrix<br>\n",
    "&nbsp; &bull; Activation function is set to ReLU define by y=max(0,x), which lends to model sparsity and faster convergence.<br>\n",
    "&nbsp; &bull; MaxPool2D picks the max value of the neighboring pool<br>\n",
    "\n",
    "The Flatten layer converts the final feature maps into a single 1D vector.\n",
    "\n",
    "Dense refers to the fact that each neuron of a layer are connected to all the neurons of the layer previous.\n",
    "\n",
    "Dropout randomly selects nodes to be ignored(set w=0), promoting distributed learning, generalization and reducing overfitting.\n",
    "\n",
    "The output layer consists of 10 nodes, representing the classification options of 0-9<br>\n",
    "&nbsp; &bull; 'softmax' has the output layer sum to 1 allowing the output nodes to be interpreted as probabilities of their corresponding number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CNN model\n",
    "\n",
    "# Architexture: Input -> [[Conv2D->ReLU]*2 -> MaxPool2D -> Dropout] * 2 -> \n",
    "#  Flatten -> Dense -> Dropout -> Output\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (img_rows,img_cols,num_channels)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settup Metrics, Loss Function and Optimizer algorithm\n",
    "\n",
    "&nbsp;&bull;&nbsp;Metric: Accuracy<br>\n",
    "&nbsp;&bull;&nbsp;Loss Funtion: Cross Entropy<br>\n",
    "&nbsp;&bull;&nbsp;Optimization: Adam (Stochastic Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(metrics=[\"accuracy\"],loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 870,634\n",
      "Trainable params: 870,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fitting the Model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      "16384/54000 [========>.....................] - ETA: 2:02 - loss: 0.4180 - accuracy: 0.8613"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c8cc36bb8380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           validation_data=(X_val, Y_val))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_trainData, Y_trainData,  \n",
    "          batch_size=64,\n",
    "          epochs=15,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
